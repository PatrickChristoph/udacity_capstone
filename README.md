# Customer Segmentation & Prediction

## Description
The project aims to enhance Bertelsmann's understanding of its customer base through a comprehensive data analysis framework. It is divided into three key components:

**Data Exploration**: This initial phase focuses on identifying and addressing data quality issues within the provided demographic data. By cleaning and refining the dataset, the project ensures that subsequent analyses are based on accurate and reliable information.

**Customer Segmentation with Unsupervised Model**: In this part, an unsupervised machine learning model is employed to categorize individuals into distinct segments based on their behaviors and characteristics, allowing the company to uncover distinct customer traits and behaviors.

**Customer Prediction with Supervised Model**: Finally, a supervised machine learning model is utilized to predict future customers, enabling Bertelsmann to effectively target future mailout campaigns based on demographic data.

Overall, the project seeks to leverage data-driven insights to support more effective marketing and customer relationship strategies.

## Project Structure
- `data/`: Contains demographic datasets used for analysis and model trainings. (will be downloaded, not part of the repository)
  - `meta/`: Contains files with meta information about the provided demographic datasets.
- `src/`: Source code for data processing, model training and evaluation.
  - `customer_prediction.ipynb`: This notebook contains the implementation of the supervised model used for predicting customer responses to future mailout campaigns.
  - `customer_segmentation.ipynb`: This notebook focuses on applying unsupervised learning techniques to segment customers based on their traits and behaviors.
  - `data_exploration.ipynb`: This notebook is dedicated to exploring the demographic datasets, identifying data quality issues and performing necessary cleaning steps to prepare the data for analysis and modeling.
  - `feature_config.json`: This JSON file defines the configuration for features used in the modeling process, specifying which data attributes are excluded or how they should be processed.
  - `files_io.py`: This Python module handles input and output operations for reading and writing data files.
  - `preprocessing.py`: This script contains functions for preprocessing the data, including cleaning, encoding and scaling the data.
- `pyproject.toml`: List of Python packages required to run the project.
- `poetry.lock`: This file is automatically generated by the Poetry dependency management tool and contains a complete list of all the dependencies and their specific versions required to run the project.

## Dependencies
This project uses Poetry for dependency management. The following dependencies are specified in the `pyproject.toml` file:

- **Python**: Use 3.12 or higher
- **Matplotlib**: Plotting library for creating visualizations
- **Openpyxl**: Reading and writing Excel files
- **Optuna**: Hyperparameter optimization framework
- **Pandas**: Data manipulation and analysis
- **Scikit-learn**: Machine learning library
- **Seaborn**: Visualization library based on Matplotlib
- **TQDM**: Progress bar for loops
- **XGBoost**: Optimized gradient boosting library

## Installation
To get started with this project, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/PatrickChristoph/udacity_capstone.git
   
2. Navigate to project directory:
   ```bash
   cd udacity_capstone

3. Install Poetry:
   ```bash
   curl -sSL https://install.python-poetry.org | python3 -

4. Install project dependencies:
   ```bash
   poetry install

## Usage
To run Jupyter Notebooks with the libraries installed via Poetry, follow these steps:
1. First, ensure you have Jupyter installed. You can add it to your Poetry environment by running:
   ```bash
   poetry add jupyter
2. Start Jupyter Notebook within the Poetry environment:
   ```bash
   poetry run jupyter notebook
3. This command will open Jupyter Notebook in your web browser. You can now create new notebooks or open existing ones, and all the libraries specified in your pyproject.toml will be available for use.

Alternatively you can possibly use Jupyter Integrations of your IDE, e.g. PyCharm. Usually, you only have to select the Python Interpreter from your created Poetry environment. You can use this command to see the environment path:
`poetry env list --full-path`

## Acknowledgments
The datasets used in this project were provided by AZ Direct GmbH. Special thanks to Bertelsmann for their contribution.

## Author
This project was developed by Patrick Christoph as part of the Udacity Data Science program.